{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What linear regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "Probably mini-batch or stochasitc gradient descent would be good here, since neither needs to operate on all the features at once.\n",
    "\n",
    "2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What might you do about it?\n",
    "\n",
    "Any gradient descent algorithm would suffer because certain features could skew the gradients simply by virtue of being on a different scale. This can be addressed by rescaling the features so that they are all on the same scale.\n",
    "\n",
    "3. Can gradient descent get stuck in a local minimum when training a logistic regression model?\n",
    "\n",
    "No, because the logistic regression cost function is convex.\n",
    "\n",
    "4. Do all gradient descent algorithms lead to the same model provided you let them run long enough?\n",
    "\n",
    "It depends on the learning rate. If this is set too high, the algorithm may never converge no matter how long it runs.\n",
    "**NB** also if the cost function has local minima you could get stuck in one of those, which could lead to different solutions.\n",
    "\n",
    "5. Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "It's likely that the model is over-fitting to the training data. This can be addressed by regularizing and/or feeding the model more training data.\n",
    "\n",
    "6. Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?\n",
    "\n",
    "Yes. Mini-batch should always be moving towards the optimal solution so if the validation error goes up that is a sign of over-fitting.\n",
    "\n",
    "**WRONG**. These algorithms are stochastic and thus their validation error may jump around a bit. A better approach is to stop once validation error has not improved after some amount of time.\n",
    "\n",
    "7. Which gradient descent algorithm (amongst those discussed) will reach the vicinity of the optimal solution fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "Stochastic will reach the vicinity fastest but may not converge. Mini-batch will be a little slower but should converge. Stochastic can be made to converge by decreasing the learning rate as a function of the number of epochs.\n",
    "**NB** batch gradient descent is also guaranteed to converge.\n",
    "\n",
    "8. Suppose you are using polynomial regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "This is likely indicative of over-fitting and can be addressed by choosing a simpler polynomial model, using some type of regularization, or re-shuffling the training/validation data to make sure the training data doesn't have some kind of bias.\n",
    "\n",
    "9. Suppose you are using ridge regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter alpha or reduce it?\n",
    "\n",
    "I would say the model has high bias, since it seems that the model is not fitting the training or validation data well, suggesting the assumptions of the model do not align with the actual properties of the data. I would decrease alpha to allow the model to better fit the data.\n",
    "\n",
    "10. Why would you want to use:\n",
    "\n",
    "* Ridge regression instead of plain linear regression (i.e. without any regularization)?\n",
    "\n",
    "This doesn't make sense, as ridge w/o regularization *IS* plain linear regression.\n",
    "\n",
    "* Lasso instead of ridge regression?\n",
    "\n",
    "Lasso is useful if you want to completely eliminate the weights of features that you think are unimportant.\n",
    "\n",
    "* Elastic net instead of lasso?\n",
    "\n",
    "You'd do this if you weren't sure whether you wanted to completely eliminate the unimportant features but did want to make sure they were down-weighted.\n",
    "**NB** Lasso may behave erratically when some of the features are correlated and/or when there are many more features  than samples.\n",
    "\n",
    "11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?\n",
    "\n",
    "I'd say use one softmax regression classifier and have it distinguish b/w the 4 possible combinations of outdoor/day, outdoor/night, indoor/day, indoor/night.\n",
    "**WRONG**. Kinda misunderstood the question. But the idea is that you don't care about distinguishing all 4 classes, just outdoor/indoor and daytime/nighttime. So in that case it makes more sense to train 2 logistic regression classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding exercise\n",
    "12. Implement batch gradient descent with early stopping for softmax regression (without using scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load datasets and libraries\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get feature (X) and target (Y) vectors\n",
    "X = iris.data\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set random seed so results are reproducible\n",
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split data into training and testing\n",
    "def train_test_val_split(x, y, test_ratio=0.2, val_ratio=0.2):\n",
    "    n = len(x) # how many items do we have?\n",
    "    train_ratio = 1 - test_ratio - val_ratio\n",
    "    train_n = int(n * train_ratio)\n",
    "    test_n = int(n * test_ratio)\n",
    "    # make a random permutation of indices from 0 to n\n",
    "    idx = np.random.permutation(len(x))\n",
    "    # split up the data\n",
    "    train_idx = idx[:train_n]\n",
    "    test_idx = idx[(train_n + 1):(train_n + 1 + test_n)]\n",
    "    val_idx = idx[(train_n + 1 + test_n + 1):n]\n",
    "    return x[train_idx], x[test_idx], x[val_idx], y[train_idx], y[test_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to convert class label vector into one-hot probablitiy encodings\n",
    "def to_one_hot(x):\n",
    "    nclasses = np.max(x) + 1\n",
    "    a = np.zeros((len(x), nclasses))\n",
    "    a[np.arange(len(x)), x] = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to compute the softmax of a vector of probabilities (aka logits)\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    sum_exps = sum(exps)\n",
    "    return exps / sum_exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross entropy cost function\n",
    "def cost_function(p_hat, y, eps=1e-7):\n",
    "    # p_hat is an array of logits\n",
    "    # y is an array of booleans indicating the true class \n",
    "    m = p_hat.shape[0] # number of observations\n",
    "    # it's ok to use regular multiplication (not matrix) here b/c y and p_hat will have the same shape\n",
    "    return -np.mean(np.sum(y * np.log(p_hat + eps), axis=1))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to train the softmax regressor using early stopping\n",
    "def train_softmax(x, y, learning_rate=0.01, eps=1e-7, niter=int(1e3), early_stop_thresh=1e-3):\n",
    "    # how many observations\n",
    "    m = x.shape[0]\n",
    "    # initialize the values for Theta, the matrix of feature weights (rows)\n",
    "    # by possible classes (columns)\n",
    "    Theta = np.random.randn(x.shape[1], y.shape[1])\n",
    "    # start iterating\n",
    "    for i in range(niter):\n",
    "            # compute the logits\n",
    "            logits = x.dot(Theta)\n",
    "            # and the probabilities for each class\n",
    "            y_proba = softmax(logits)\n",
    "            # value of the cost function\n",
    "            loss = cost_function(y_proba, y, eps=eps)\n",
    "            # what is the error of our predictions?\n",
    "            error = y - y_proba\n",
    "            # some reporting\n",
    "            if i % 500 == 0:\n",
    "                print(i, loss)\n",
    "            if loss <= early_stop_thresh:\n",
    "                # reached our minimum loss tolerance, stop early\n",
    "                print(\"Min. loss reached, stopping\")\n",
    "                return Theta\n",
    "            # compute the gradients\n",
    "            gradients = (1/m) * x.T.dot(error)\n",
    "            # update Theta\n",
    "            Theta = Theta - learning_rate * gradients\n",
    "    return Theta\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to add a bias term to every entry of X\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "\n",
    "# make the training, testing, validation datasets\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = train_test_val_split(X_with_bias, Y, test_ratio=0.2, val_ratio=0.1)\n",
    "\n",
    "# convert the class encodings to one-hot\n",
    "Y_train_one_hot = to_one_hot(Y_train)\n",
    "Y_test_one_hot = to_one_hot(Y_test)\n",
    "Y_val_one_hot = to_one_hot(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([[0, 4, 3, 5, 6, 1], [0, 5, 2, 5, 3, 1]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_softmax(X_train, Y_train_one_hot, niter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
